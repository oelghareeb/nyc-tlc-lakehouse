{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c6d8ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, hour, minute, second, date_format, sha2, concat_ws\n",
    "\n",
    "# -----------------------------\n",
    "# DEFINE SENSITIVE VARIABLES\n",
    "# -----------------------------\n",
    "CATALOG_URI = \"http://nessie:19120/api/v1\"      # Nessie Server URI\n",
    "WAREHOUSE = \"s3://lakehouse/\"                   # MinIO Iceberg warehouse\n",
    "MINIO_ENDPOINT = \"http://minio:9000\"            # MinIO endpoint\n",
    "MINIO_ACCESS_KEY = \"admin\"                      # MinIO access key\n",
    "MINIO_SECRET_KEY = \"password\"                   # MinIO secret key\n",
    "RAW_BUCKET = \"lakehouse/raw/\"                   # Bucket where Parquet files are stored\n",
    "\n",
    "# MONTHS = [f\"2025-{str(m).zfill(2)}\" for m in range(1, 4)] # adjust the range of the files if needed\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURE SPARK\n",
    "# -----------------------------\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "    .setAppName('yellow_tripdata_app')\n",
    "    .set(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,\"\n",
    "        \"software.amazon.awssdk:bundle:2.24.8,\"\n",
    "        \"software.amazon.awssdk:url-connection-client:2.24.8,\"\n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.4\"\n",
    "    )\n",
    "    .set(\"spark.sql.extensions\",\n",
    "         \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,\"\n",
    "         \"org.projectnessie.spark.extensions.NessieSparkSessionExtensions\")\n",
    "    .set(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .set(\"spark.sql.catalog.nessie.uri\", CATALOG_URI)\n",
    "    .set(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .set(\"spark.sql.catalog.nessie.authentication.type\", \"NONE\")\n",
    "    .set(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .set(\"spark.sql.catalog.nessie.warehouse\", WAREHOUSE)\n",
    "    .set(\"spark.sql.catalog.nessie.s3.endpoint\", MINIO_ENDPOINT)\n",
    "    .set(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    .set(\"spark.sql.catalog.nessie.s3.access-key\", MINIO_ACCESS_KEY)\n",
    "    .set(\"spark.sql.catalog.nessie.s3.secret-key\", MINIO_SECRET_KEY)\n",
    "    .set(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\")\n",
    "    .set(\"spark.sql.catalog.nessie.s3.region\", \"us-east-1\")\n",
    "    .set(\"spark.sql.catalog.nessie.s3.connection-ssl-enabled\", \"false\")\n",
    "\n",
    "    # S3A driver for Spark\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .set(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "         \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    ")\n",
    "\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print(\"ðŸ”¥ Spark Session Started\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1. CREATE ICEBERG NAMESPACE\n",
    "# -----------------------------\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.nyc_bronze;\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Read Parquet files from S3\n",
    "# -----------------------------\n",
    "# Optionally, you can read all files and filter by month later\n",
    "df = spark.read.parquet(\"s3a://lakehouse/raw/*.parquet\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Transform the DataFrame\n",
    "# -----------------------------\n",
    "df_transformed = df \\\n",
    "    .withColumn(\"pickup_date_id\", date_format(col(\"tpep_pickup_datetime\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"dropoff_date_id\", date_format(col(\"tpep_dropoff_datetime\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"pickup_time_id\", hour(col(\"tpep_pickup_datetime\"))*3600 + \n",
    "                                  minute(col(\"tpep_pickup_datetime\"))*60 + \n",
    "                                  second(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .withColumn(\"dropoff_time_id\", hour(col(\"tpep_dropoff_datetime\"))*3600 + \n",
    "                                   minute(col(\"tpep_dropoff_datetime\"))*60 + \n",
    "                                   second(col(\"tpep_dropoff_datetime\"))) \\\n",
    "    .withColumn(\"trip_id\", sha2(concat_ws(\"_\", col(\"tpep_pickup_datetime\"), col(\"vendorid\"), col(\"ratecodeid\")), 256))\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Write month by month to Iceberg\n",
    "# -----------------------------\n",
    "months = df_transformed.select(date_format(col(\"tpep_pickup_datetime\"), \"yyyy-MM\").alias(\"month\")) \\\n",
    "                       .distinct() \\\n",
    "                       .collect()\n",
    "\n",
    "for month_row in months:\n",
    "    month = month_row[\"month\"]\n",
    "    print(f\"Processing month: {month}\")\n",
    "\n",
    "    # Filter only rows for this month\n",
    "    df_month = df_transformed.filter(date_format(col(\"tpep_pickup_datetime\"), \"yyyy-MM\") == month)\n",
    "    \n",
    "    # Write to Iceberg table\n",
    "    df_month.write \\\n",
    "        .format(\"iceberg\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"write-format\", \"parquet\") \\\n",
    "        .option(\"partition-spec\", \"months(tpep_pickup_datetime)\") \\\n",
    "        .saveAsTable(\"nessie.nyc_bronze.yellow_tripdata_raw\")\n",
    "\n",
    "    print(f\"âœ… Finished writing month: {month}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Verify\n",
    "# -----------------------------\n",
    "spark.sql(\"SELECT * FROM nessie.nyc_bronze.yellow_tripdata_raw LIMIT 5\").show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
